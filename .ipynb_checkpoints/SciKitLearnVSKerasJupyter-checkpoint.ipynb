{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At the beginning importing all of the classes and functions needed for the project\n",
    "\n",
    "#Including both the functionality required Keras and data loading from pandas. \n",
    "#Also data preparation and model evaluation from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris #sklearn contains many databases, one of them is Iris. makes it very easy to download dataset.\n",
    "from sklearn.neighbors import KNeighborsClassifier #Classifier that implements the k-nearest neighbors vote.\n",
    "from sklearn.svm import SVC #Linear Support Vector Classification\n",
    "from sklearn import tree #decision trees are a non-parametric supervised learning method which is used for classification and regression\n",
    "from sklearn.linear_model import LogisticRegression #The SVM module is an addition to libsvm library and it supports different kernels \n",
    "from sklearn.ensemble import RandomForestClassifier #Meta estimator, fits a number of decision tree classifiers \n",
    "import warnings # In order to ignore FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from keras.callbacks import TensorBoard #Training Run: Additionally use Tensorboard as a callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#show dataset\n",
    "data = load_iris()\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataset can be loaded directly using sklearn datasets. \n",
    "\n",
    "data.target[[10, 25, 50]]\n",
    "array=([0, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#present number of observations (150), number of features (4)\n",
    "\n",
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f2cecd46311d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#shape of train and test object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(X, y, \n\u001b[0m\u001b[0;32m      5\u001b[0m \ttest_size = 0.33, random_state = 42)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Spliting data and training the model\n",
    "#shape of train and test object\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "\ttest_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-02ae856cc2e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# k-nearest-neighbour classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  An explanation and code for the iris species based on the dataset using scikit-learn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;31m#data is like pandas dataframe, input variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;31m#target is like pandas Series, output variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iris' is not defined"
     ]
    }
   ],
   "source": [
    "# k-nearest-neighbour classifier \n",
    "#  An explanation and code for the iris species based on the dataset using scikit-learn.\n",
    "x = iris.data #data is like pandas dataframe, input variables\n",
    "y = iris.target #target is like pandas Series, output variables\n",
    "\n",
    "#define a function that prints the iris' classification based on the algorithm's output\n",
    "def classifyiris(p):\n",
    "    if p[0] == 0:\n",
    "        print(\"The iris is setosa.\\n\")\n",
    "    elif p[0] == 1:\n",
    "        print(\"The iris is versicolor.\\n\")\n",
    "    else:\n",
    "        print(\"The iris is virginica.\\n\")\n",
    "\n",
    "#Using the K Nearest Neighbor Algorithm\n",
    "# Declare an of the KNN classifier class with the value with neighbors.\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "knn.fit(x,y)\n",
    "\n",
    "p = knn.predict([[3,5,4,2]])\n",
    "print(\"Using the k nearest neighbor algorithm =\", knn.predict([[3,5,4,2]]))\n",
    "classifyiris(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_67 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_custom_model(input_dim, output_dim, nodes, n=1, name='model'):\n",
    "    def create_model():\n",
    "        # Create model\n",
    "        model = Sequential(name=name)\n",
    "        for i in range(n):\n",
    "            model.add(Dense(nodes, input_dim=input_dim, activation='relu'))\n",
    "        model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return create_model\n",
    "\n",
    "models = [create_custom_model(n_features, n_classes, 8, i, 'model_{}'.format(i)) \n",
    "          for i in range(1, 4)]\n",
    "\n",
    "for create_model in models:\n",
    "    create_model().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# One hot encoding\n",
    "enc = OneHotEncoder()\n",
    "Y = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y, test_size=0.5, random_state=2)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "n_classes = Y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: model_1\n",
      "Test loss: 0.31701013445854187\n",
      "Test accuracy: 0.8799999952316284\n",
      "Model name: model_2\n",
      "Test loss: 0.2020597904920578\n",
      "Test accuracy: 0.9333333373069763\n",
      "Model name: model_3\n",
      "Test loss: 0.5639191269874573\n",
      "Test accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "history_dict = {}\n",
    "\n",
    "# TensorBoard Callback\n",
    "cb = TensorBoard()\n",
    "\n",
    "for create_model in models:\n",
    "    model = create_model()\n",
    "    print('Model name:', model.name)\n",
    "    history_callback = model.fit(X_train, Y_train,\n",
    "                                 batch_size=5,\n",
    "                                 epochs=50,\n",
    "                                 verbose=0,\n",
    "                                 validation_data=(X_test, Y_test),\n",
    "                                 callbacks=[cb])\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    history_dict[model.name] = [history_callback, model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
